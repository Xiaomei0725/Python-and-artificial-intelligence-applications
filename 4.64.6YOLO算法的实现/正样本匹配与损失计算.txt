正样本匹配：
    def build_targets(self, p, targets):
        '''
        所有GT筛选相应的anchor正样本
        :param p: 预测信息(feature_map输出)
                list, 存放三个列表, 如输入为(4, 3, 640, 640)
                {[4, 3, 80, 80, 85], [4, 3, 40, 40, 85], [4, 3, 20, 20, 85]}
                [bs, na, grid_y, grid_x, xywh(回归参数)+class+classes]
        :param targets: 当前batch中的真实框 [nt, 6] [image_index, classes, xywh(相对坐标)]
        :return:tcls: 正样本类别
                tbox: 正样本位置(xywh) 其中xy为这个target对当前grid_cell左上角偏移量, xywh均为当前特征图上的绝对坐标
                indices: b:表示正样本的image index
                         a:表示正样本使用的anchor index
                         gj: 表示正样本的预测单元左上角y坐标
                         gi: 表示正样本的预测单元左上角x坐标
                anch: 表示正样本使用的anchor的尺度(相对于feature map)
        '''
        na, nt = self.na, targets.shape[0]  # anchor数量, 当前图片中的标签数
        tcls, tbox, indices, anch = [], [], [], []  # 存储类别、位置、索引、Anchor尺度

        # gain是为了后面将targets=[na, nt, t]中的相对坐标xywh映射到feature map上(绝对坐标)
        # image_index + class + xywh + anchor_index
        gain = torch.ones(7, device=self.device)

        # ai代表3个anchor上的所有target对应的anchor索引
        # [1, 3](0, 1, 2) -> [3, 1] -> [3, nt] 第一行nt个0, 第二行nt个1, 第三行nt个2
        ai = torch.arange(na, device=self.device).float().view(na, 1).repeat(1, nt)

        # 对一个feature map:这一步操作将target复制三份, 每一份对应一个feature map的一个anchor
        # 先假设所有的target的由三个anchor进行匹配, 再进行筛选, 将ai加进去用于标记当前target匹配的anchor_index
        # [nt, 6] [3, nt] -> [3, nt, 6] [3, nt, 1] -> [3, nt, 7] 7:image_index+class+xywh+anchor_index
        targets = torch.cat((targets.repeat(na, 1, 1), ai[..., None]), dim=2)

        # 以下两个参数用于扩展正样本, 一个target可能有多个cell预测到(上下左右, 3个anchor, 3个feature, 3个cell, 最多有3×3×3个anchor进行匹配)
        g = 0.5  # 中心偏移, 用于配上或下cell以及左或右cell
        # 以自身+周围上下左右4个网格 = 5个网格 来计算offsets, 最后的grid要减去偏移量
        off = torch.tensor(
            [
                [0, 0],
                [1, 0],   # j 左边(x-1)
                [0, 1],   # k 上边(y-1)
                [-1, 0],  # l 右边(x+1)
                [0, -1],  # m 下边(y+1)
            ],
            device=self.device).float() * g  # offsets
        # 遍历三个feature map, 筛选正样本
        for i in range(self.nl):  # nl: 输出特征层数量
            # 当前feature map对应的anchor尺寸 [3, 2]
            anchors, shape = self.anchors[i], p[i].shape

            # gain增益, 保存当前feature map的宽和高: gain[2:6] = gain[w, h, w, h]
            # gain用于将target上的xywh相对坐标转换为feature_map上的绝对坐标
            gain[2:6] = torch.tensor(shape)[[3, 2, 3, 2]]  # (1, 1, w, h, w, h, 1)

            t = targets * gain  # [3, nt, 7] 7:image_index + class + xywh(特征图上的绝对坐标) + anchor_index

            # 若存在正样本, 则开始匹配
            if nt:
                # t[:, :, 4:6] shape [3, nt, 2], anchors[:, None] shape[3, 1, 2]
                # r[3, nt, 2]
                # 所有的gt与当前层的三个anchor的宽高比(w / w, h / h)
                r = t[..., 4:6] / anchors[:, None]

                # 正样本筛选条件 GT与anchor的宽比或高比超过一定的阈值, 就当作负样本
                # torch.max(r, 1. / r) = [3, 63, 2] 筛选出宽比w1/w2, w2/w1和高比h1/h2, h2/h1中最大的那个
                # .max(dim=2)返回宽比、高比两者中较大的一个值和其索引, [0]为返回值, [1]为返回索引
                # j [3, nt] 小于anchor_t的为正样本 True:正样本, False:负样本
                j = torch.max(r, 1 / r).max(2)[0] < self.hyp['anchor_t']

                # 根据筛选条件j, 过滤负样本, 得到所有gt的anchor正样本
                # 知道gt的坐标属于哪张图片正样本对应的idx, 也就得到了当前正样本anchor
                # t [3, nt, 7], j[3, nt](假设其中42个为True) -> [126, 7]
                t = t[j]

                # offsets筛选当前格子周围格子, 找到两个离target中心最近的两个格子, 可能周围的格子也预测到了当前样本
                # 除了target所在的当前格子, 还有2个格子对目标进行检测(计算损失)
                # 利用中心坐标对1求余与g比较, 判断选择哪两个格子
                # gxy:[126, 2] gain[[2, 3]]:[1,2]
                gxy = t[:, 2:4]  # grid xy 取目标中心的坐标(绝对坐标,相对于feature map左上角的坐标)
                gxi = gain[[2, 3]] - gxy  # (绝对坐标, 相对于feature map右下角的坐标)

                # j: [126] 如果是True表示当前target中心点所在的格子的左边格子也对该target进行回归
                # k: [126] 如果是True表示当前target中心点所在大的格子的上边格子也对该target进行回归
                j, k = ((gxy % 1 < g) & (gxy > 1)).T

                # l: [126] 如果是True表示当前target中心点所在的格子的右边格子也对该target进行回归
                # m: [126] 如果是True表示当前target中心点所在的格子的下边格子也对该target进行回归
                l, m = ((gxi % 1 < g) & (gxi > 1)).T

                # j [5, 126] torch.ones_like(j): 当前格子不需要筛选均为True, j,k,m,l:左上右下格子的筛选结果
                j = torch.stack((torch.ones_like(j), j, k, l, m))
                # 得到筛选后的所有格子正样本 格子数 <= 3 * 126, 不在边上时等号成立
                # t复制5份, 分别对应五个格子
                # t [126, 7]->[5, 126, 7]; j[5, 126]
                # t[5, 126, 7] + j[5, 126] = t[378, 7] 理论上小于等于3倍的126, 当且仅当没有边界格子时等号成立
                t = t.repeat((5, 1, 1))[j]

                # gxy:[126, 2] torch.zeros_like(gxy)[None]: [1, 126, 2]
                # off:[5,2]  off[:, None]:[5, 1, 2]  off[:, None][j] = [5, 126, 2] + [5, 126] = [378, 2] 广播机制
                # offsets: [378, 2]得到所有筛选后的网格的中心相对于这个要预测的真实框所在网格边界（左右上下边框）的偏移量
                offsets = (torch.zeros_like(gxy)[None] + off[:, None])[j]
            else:
                t = targets[0]
                offsets = 0
            # t[378, 7] t.chunk(4, dim=1)在维度1上划分为4个块 (image_index, class, x, y, w, h, anchor_index)
            # bc:(image_index, class) gxy gwh a(anchor_index)
            bc, gxy, gwh, a = t.chunk(4, 1)
            # a:anchor_index, b:image_index, c:class
            a, (b, c) = a.long().view(-1), bc.long().T
            gij = (gxy - offsets).long()  # 预测真实框的网络所在的左上角坐标, 其中.long()化为长整型进行了取整 [378, 2]
            gi, gj = gij.T  # grid xy

            # b:image_index, a:anchor_index, gj:网格左上角y坐标, gi:网格的左上角x坐标
            indices.append((b, a, gj.clamp_(0, shape[2] - 1), gi.clamp_(0, shape[3] - 1)))
            # xywh(绝对坐标), 其中xy为target对当前grid_cell左上角的偏移量
            tbox.append(torch.cat((gxy - gij, gwh), 1))
            # 对应所有的anchors
            anch.append(anchors[a])
            tcls.append(c)

        return tcls, tbox, indices, anch
IoU计算：
def bbox_iou(box1, box2, xywh=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-7):
    '''
    计算Iou/GIou/DIou/CIou,默认计算普通IoU
    '''

    # 将xywh坐标形式转换成xyxy(左上角右下角)坐标形式,便于计算面积与对角线长度
    if xywh:  # transform from xywh to xyxy
        (x1, y1, w1, h1), (x2, y2, w2, h2) = box1.chunk(4, -1), box2.chunk(4, -1)
        w1_, h1_, w2_, h2_ = w1 / 2, h1 / 2, w2 / 2, h2 / 2
        b1_x1, b1_x2, b1_y1, b1_y2 = x1 - w1_, x1 + w1_, y1 - h1_, y1 + h1_
        b2_x1, b2_x2, b2_y1, b2_y2 = x2 - w2_, x2 + w2_, y2 - h2_, y2 + h2_
    else:  # x1, y1, x2, y2 = box1
        b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)
        b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)
        w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps
        w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps

    # 计算交集面积
    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \
            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)

    # 计算并集面积
    union = w1 * h1 + w2 * h2 - inter + eps
	
	# 计算IoU
    iou = inter / union
    if CIoU or DIoU or GIoU:
    	# 计算包围两个矩形框的最小框宽和高
        cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)  # 最小包围矩形框宽度
        ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)  # 最小包括矩形框高度
        if CIoU or DIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1
            c2 = cw ** 2 + ch ** 2 + eps  # 最小包围矩形框对角线长度
            rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # 矩形框中心点连线长度
            if CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47
                v = (4 / math.pi ** 2) * torch.pow(torch.atan(w2 / h2) - torch.atan(w1 / h1), 2)
                with torch.no_grad():
                    alpha = v / (v - iou + (1 + eps))
                return iou - (rho2 / c2 + v * alpha)  # CIoU
            return iou - rho2 / c2  # DIoU
        c_area = cw * ch + eps  # 最小包围矩形框面积
        return iou - (c_area - union) / c_area  # GIoU https://arxiv.org/pdf/1902.09630.pdf
return iou  # IoU